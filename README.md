# Fitness-Activity-Recognition
![image](https://user-images.githubusercontent.com/86193960/215552963-fb5cb0c0-05a0-41d6-bdfb-245214aa7166.png)


Whole-body fitness activity recognition, focusing on sports exercise, is our main goal for this paper. Our work is able to recognize fitness activities without any external hardware component but a video. In this work, we tested different processing methods on the processing methods of seven sports activity classes: aerobics, weight_bearing, calisthenics, weightlifting, coordination_agility, balance_stability, and flexibility exercises. Examples of the activities. In this paper, we proposed the use of MoveNet model that detects 17 key points of the human body, BlazePose model is specifically designed to detect the body pose for fitness activities and detects 33 key points, and the RepNet model that takes input video that contains periodic action of a variety of classes and returns the period of repetitions found so our proposed application uses RepNet model for counting each class repetitions in videos.

We chose these activities because they are all periodic movements and can be performed on the same spot, without changing the physical setup also we can use RepNet to detect the repetitions of each periodic activity. Fitness activity recognition is the problem of predicting the movement of a person and the classification of actions that are performed, often indoors, based on the body poses, such as the 33 key points gained from BlazePose model. Movements are often typical activities performed indoors, such as walking, working out, doing yoga, standing, and practicing different activities.

In this work, we present a simple approach to leveraging different models to increase the flexibility and generality of fitness activity recognition models. Our work is inspired by the BlazePose model which performs so well in yoga and fitness use cases. The proposed system profits from RepNet architecture that uses the ResNet as per frame model to generate embeddings of each frame of the video then passing each frame of a video through a ResNet-based encoder yields a sequence of embeddings. At this point we calculate a temporal self-similarity matrix (TSM) by comparing each frameâ€™s embedding with every other frame in the video, returning a matrix that is easy for subsequent modules to analyze for counting repetitions. For each frame, we then use Transformers to predict the period of repetition and the periodicity directly from the sequence of similarities in the TSM. Once we have the period, we obtain the per-frame count by dividing the number of frames captured in a periodic segment by the period length. We sum this up to predict the number of repetitions in the video.
